# -*- coding: utf-8 -*-
"""protosquad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2Nvf6wbf68hXu7-AkqCBXgs2-bXoW3I
"""

# StudyMate AI - Academic Assistant with Chat, Translation & Image Analysis
# Ready-to-run in Google Colab - Single Cell Implementation
# Complete with Answer Button - FINAL VERSION

# Install required packages
!pip install -q gradio transformers torch PyPDF2 pillow deep-translator accelerate bitsandbytes sentencepiece protobuf

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from PIL import Image
import PyPDF2
import io
from deep_translator import GoogleTranslator
import warnings
warnings.filterwarnings('ignore')

# Initialize model and tokenizer
print("üîÑ Loading IBM Granite 3.3B Instruct model...")
model_name = "ibm-granite/granite-3.0-3b-a800m-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True,
    trust_remote_code=True
)
print("‚úÖ Model loaded successfully!")

# Initialize vision model for image analysis
print("üîÑ Loading vision model...")
vision_pipeline = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base", device=0 if torch.cuda.is_available() else -1)
print("‚úÖ Vision model loaded successfully!")

# Global variable to store PDF content
pdf_content = ""

# Supported Indian languages
LANGUAGES = {
    "English": "en",
    "Hindi": "hi",
    "Tamil": "ta",
    "Telugu": "te",
    "Kannada": "kn",
    "Malayalam": "ml",
    "Bengali": "bn",
    "Marathi": "mr",
    "Gujarati": "gu",
    "Punjabi": "pa"
}

def extract_pdf_text(pdf_file):
    """Extract text from uploaded PDF"""
    global pdf_content

    if pdf_file is None:
        return "‚ö†Ô∏è Please upload a PDF file first!"

    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file.name)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        pdf_content = text

        if len(text.strip()) < 50:
            return f"‚ö†Ô∏è PDF uploaded but extracted text is limited ({len(text)} characters)."

        return f"‚úÖ PDF uploaded successfully!\nüìÑ Pages: {len(pdf_reader.pages)}\nüìù Characters: {len(text):,}\n\nüí° Now ask questions about your document!"
    except Exception as e:
        return f"‚ùå Error reading PDF: {str(e)}"

def generate_answer(question, history):
    """Generate answer based on PDF content - Called by Answer button"""
    global pdf_content

    # Check if question is empty
    if not question or question.strip() == "":
        return history, ""

    # Check if PDF is uploaded
    if not pdf_content:
        history = history or []
        history.append((question, "‚ö†Ô∏è Please upload a PDF document first!"))
        return history, ""

    try:
        # Create context-aware prompt
        context = pdf_content[:4000] if len(pdf_content) > 4000 else pdf_content

        prompt = f"""Based on the following document content, answer the question accurately and concisely.

Document Content:
{context}

Question: {question}

Answer:"""

        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the answer part
        if "Answer:" in response:
            answer = response.split("Answer:")[-1].strip()
        else:
            answer = response.split(question)[-1].strip()

        # Clean up answer
        if len(answer) < 10:
            answer = "I couldn't find enough information in the document to answer this question. Try rephrasing or asking about a different topic."

        # Append to history
        history = history or []
        history.append((question, answer))

        return history, ""  # Return history and clear textbox

    except Exception as e:
        history = history or []
        history.append((question, f"‚ùå Error: {str(e)}"))
        return history, ""

def clear_chat():
    """Clear chat history"""
    return [], ""

def translate_text(text, source_lang, target_lang):
    """Translate text between languages"""
    if not text or text.strip() == "":
        return "‚ö†Ô∏è Please enter text to translate."

    try:
        if source_lang == target_lang:
            return text

        source_code = LANGUAGES[source_lang]
        target_code = LANGUAGES[target_lang]

        translator = GoogleTranslator(source=source_code, target=target_code)
        translated = translator.translate(text)

        return translated
    except Exception as e:
        return f"‚ùå Translation error: {str(e)}"

def analyze_image(image):
    """Analyze uploaded image and provide description"""
    if image is None:
        return "‚ö†Ô∏è Please upload an image first!"

    try:
        # Generate image caption
        result = vision_pipeline(image)
        caption = result[0]['generated_text']

        # Use Granite model for detailed analysis
        prompt = f"""Analyze this image in detail for educational purposes. The image shows: {caption}

Provide a detailed academic analysis including:
1. Main subjects or concepts visible
2. Relevant educational context
3. Key observations for study purposes

Analysis:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=250,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        analysis = response.split("Analysis:")[-1].strip()

        return f"üìä **Quick Description:** {caption}\n\nüìö **Detailed Analysis:**\n{analysis}"

    except Exception as e:
        return f"‚ùå Error analyzing image: {str(e)}"

# Create Gradio Interface
with gr.Blocks(title="StudyMate AI - Academic Assistant", theme=gr.themes.Soft()) as app:

    gr.Markdown("""
    # üìö StudyMate AI - Your Academic Assistant
    ### Features: PDF Q&A | Language Translation | Image Analysis
    """)

    with gr.Tabs():
        # Tab 1: PDF Chatbot with Answer Button
        with gr.Tab("üí¨ PDF Chatbot"):
            gr.Markdown("### Upload your study materials and ask questions!")

            with gr.Row():
                pdf_upload = gr.File(label="Upload PDF Document", file_types=[".pdf"])
                pdf_status = gr.Textbox(label="Upload Status", interactive=False, lines=4)

            pdf_upload.change(extract_pdf_text, inputs=pdf_upload, outputs=pdf_status)

            gr.Markdown("---")

            chatbot = gr.Chatbot(label="Chat with your PDF", height=400)

            msg = gr.Textbox(
                label="Ask a question about your document",
                placeholder="What is the main concept discussed in chapter 2?",
                lines=2
            )

            with gr.Row():
                answer_btn = gr.Button("üéØ Get Answer", variant="primary", size="lg", scale=2)
                clear_btn = gr.Button("üóëÔ∏è Clear Chat", variant="secondary", size="lg", scale=1)

            # Connect buttons to functions
            answer_btn.click(
                generate_answer,
                inputs=[msg, chatbot],
                outputs=[chatbot, msg]
            )

            msg.submit(
                generate_answer,
                inputs=[msg, chatbot],
                outputs=[chatbot, msg]
            )

            clear_btn.click(clear_chat, outputs=[chatbot, msg])

            gr.Examples(
                examples=[
                    "What are the key points in this document?",
                    "Summarize the main concepts",
                    "Explain the methodology described",
                    "What are the conclusions?"
                ],
                inputs=msg
            )

        # Tab 2: Language Translation
        with gr.Tab("üåê Language Translation"):
            gr.Markdown("### Translate text between English and Indian languages")

            with gr.Row():
                source_lang = gr.Dropdown(
                    choices=list(LANGUAGES.keys()),
                    value="English",
                    label="Source Language"
                )
                target_lang = gr.Dropdown(
                    choices=list(LANGUAGES.keys()),
                    value="Hindi",
                    label="Target Language"
                )

            input_text = gr.Textbox(
                label="Enter text to translate",
                placeholder="Type or paste your text here...",
                lines=5
            )

            translate_btn = gr.Button("üîÑ Translate", variant="primary")

            output_text = gr.Textbox(
                label="Translation",
                lines=5,
                interactive=False
            )

            translate_btn.click(
                translate_text,
                inputs=[input_text, source_lang, target_lang],
                outputs=output_text
            )

            gr.Examples(
                examples=[
                    ["Hello, how are you?", "English", "Hindi"],
                    ["What is photosynthesis?", "English", "Tamil"],
                    ["Machine learning is fascinating", "English", "Telugu"]
                ],
                inputs=[input_text, source_lang, target_lang]
            )

        # Tab 3: Image Analysis
        with gr.Tab("üñºÔ∏è Image Analysis"):
            gr.Markdown("### Upload images from textbooks, diagrams, or charts for analysis")

            image_input = gr.Image(label="Upload Image", type="pil")
            analyze_btn = gr.Button("üîç Analyze Image", variant="primary")
            image_output = gr.Textbox(label="Analysis Results", lines=10, interactive=False)

            analyze_btn.click(analyze_image, inputs=image_input, outputs=image_output)

            gr.Markdown("""
            **Tip:** Upload diagrams, charts, equations, or any educational images for detailed analysis!
            """)

    gr.Markdown("""
    ---
    ### üéì StudyMate AI - Powered by IBM Granite 3.3B Instruct
    **Tips for best results:**
    - Upload clear, text-based PDFs for accurate Q&A
    - Be specific with your questions
    - Use high-quality images for better analysis
    """)

# Launch the app
print("\n" + "="*60)
print("üöÄ Launching StudyMate AI...")
print("="*60)
app.launch(share=True, debug=True)